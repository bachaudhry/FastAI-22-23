{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMAHpl6YZ8iBuFmTsX3aJfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachaudhry/FastAI-22-23/blob/main/course_part_2/simple_diffusion_audio_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating Audio Samples Using Simple Diffusion**"
      ],
      "metadata": {
        "id": "SIR7Q3pjw-Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Johnathan Whitaker's demo borrows heavily from the [Diffusion for Audio NB by HuggingFace](https://github.com/huggingface/diffusion-models-class/blob/main/unit4/02_diffusion_for_audio.ipynb).\n",
        "\n",
        "Additionally, Johno also created the dataset used in this NB.\n",
        "\n",
        "Diffusers Version 0.24.0 will be used given the Audio Diffusers pipeline has been deprecated [as highlighted here](https://github.com/huggingface/diffusers/pull/6169)."
      ],
      "metadata": {
        "id": "ObrO9ma6AeeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjrb_a11vdfU",
        "outputId": "046d5d07-17a8-402e-9876-aea304a0bb18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q miniminiai datasets torchaudio diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random, os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchaudio\n",
        "from torchaudio import transforms as T\n",
        "from torch.utils.data import default_collate\n",
        "from torchvision.transforms import functional as TF\n",
        "from huggingface_hub import hf_hub_download\n",
        "#from diffusers.pipelines.audio_diffusion.mel import Mel\n",
        "\n",
        "import fastcore.all as fc\n",
        "from PIL import Image\n",
        "from miniminiai import *\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "from IPython.display import Audio\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "XwJdoGv9xTUn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p2AcjuzVTRrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing core Mel Spectrogram logic\n",
        "class MelSpec:\n",
        "  def __init__(self, sample_rate=16000, x_res=None, y_res=None,\n",
        "               n_fft=1024, hop_length=512, f_min=0.0, fmax=None):\n",
        "    self.sample_rate = sample_rate\n",
        "    self.x_res = x_res\n",
        "    self.y_res = y_res\n",
        "    self.n_fft = n_fft\n",
        "    self.hop_length = hop_length\n",
        "    self.f_min = f_min\n",
        "    self.fmax = fmax\n",
        "\n",
        "    # Calculate hop_length and n_fft based on resolutions\n",
        "    self.hop_length = (sample_rate * (self.x_res - 1) + self.n_fft) // self.x_res\n",
        "    self.n_fft = self.hop_length * (self.x_res - 1) + 1\n",
        "\n",
        "    self.mel_spectrogram = T.MelSpectrogram(\n",
        "        sample_rate=self.sample_rate,\n",
        "        n_fft=self.n_fft,\n",
        "        n_mels=self.y_res,\n",
        "        hop_length=self.hop_length,\n",
        "        f_min=self.f_min,\n",
        "        f_max=self.fmax\n",
        "    )\n",
        "\n",
        "  def __call__(self, waveform):\n",
        "    mel_spec = self.mel_spectrogram(waveform)\n",
        "    mel_spec = torchaudio.functional.amplitude_to_db(mel_spec, top_db=80.0)\n",
        "    return mel_spec\n",
        "\n",
        "  def x_res(self, waveform_length):\n",
        "    return 1 + (waveform_length - self.n_fft) // self.hop_length\n",
        "\n",
        "  def y_res(self):\n",
        "    return self.n_mels"
      ],
      "metadata": {
        "id": "G1hEFRHETP0A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Audio Data**"
      ],
      "metadata": {
        "id": "RuvlppZgyqmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These settings are used for the Mel Spectrograms\n",
        "sample_rate = 16000\n",
        "x_res = 128 # x resolution of the spectrogram (time)\n",
        "y_res = 128 # y resolution of the spectrogram (frequency - binned)\n",
        "mel = MelSpec(sample_rate, x_res, y_res)"
      ],
      "metadata": {
        "id": "CXwhW1pFxTfK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset of bird calls\n",
        "birdcall_dataset = load_dataset(\"tglcourse/5s_birdcall_samples_top20\")"
      ],
      "metadata": {
        "id": "U9_HkfVbxTn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "birdcall_dataset"
      ],
      "metadata": {
        "id": "HxjlnYDSxTtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVwBPXgLxTyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIOB4MFuxT3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DeX5bjrDxT8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZs3_hiixUB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w35ogtFDxUG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeQMvUQkxUL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5iRTrKFxUQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DY7vrmFRxUV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_g0abg9xUap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}