{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7589b1d9-4782-45fc-8ab8-3a554bf7d20f",
   "metadata": {},
   "source": [
    "# **Introducing The Attention Component of Transformers** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00eedd-d04c-4268-9cfc-ac0e47a0ceb6",
   "metadata": {},
   "source": [
    "According to Perplexity **Attention**, which made a huge splash in this [seminal paper](https://arxiv.org/abs/1706.03762), can briefly be defined as: \n",
    "\n",
    ">The attention mechanism mimics human cognitive processes by allowing a model to prioritize certain inputs over others based on their relevance to the task at hand. This is particularly useful in scenarios where the input data is large and complex, enabling the model to selectively concentrate on the most pertinent elements while ignoring less relevant information.\n",
    ">\n",
    "> ### Key Concepts\n",
    ">\n",
    ">    **Encoder-Decoder Architecture**: The attention mechanism is often employed within an encoder-decoder framework. The encoder processes the input sequence and generates a set of hidden states, while the decoder uses these states to produce the output sequence. Traditional models would pass only the final hidden state from the encoder to the decoder, but attention allows for all hidden states to be considered1\n",
    ">    \n",
    ">    **Attention Weights**: The mechanism assigns weights to different parts of the input, indicating their relative importance. These weights are dynamically calculated during model training and are used to create a context vector that emphasizes significant input elements.\n",
    ">\n",
    "> ### How Attention Works\n",
    ">\n",
    ">    **Calculating Attention Scores**: For each element in the output sequence, attention scores are computed by comparing it with all elements in the input sequence. This can be done using methods such as dot-product or additive attention, where each score reflects how relevant an input element is to a particular output element.\n",
    ">\n",
    ">    **Creating Context Vectors**: The attention scores are normalized using a softmax function to produce a probability distribution. This distribution is then used to compute a weighted sum of the input elements, resulting in a context vector that highlights important features.\n",
    ">\n",
    ">    **Decoding Process**: The context vector is fed into the decoder alongside its current hidden state. This allows the decoder to generate output tokens based on both the immediate context and relevant parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28663616-262e-4899-9f57-95c9c5cf914a",
   "metadata": {},
   "source": [
    "It is important to note that Stable Diffusion's implementation of Attention is quite sub-optimal. We may consider moving to better approaches in later NBs.\n",
    "\n",
    "Initially, we will focus on **1d-Attention**, which was predominantly used for NLP. For Stable Diffusion, we will flatten all pixel rows into single vectors for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bdce3b-469b-4a38-99d8-21b9bbbc9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "from torch import nn\n",
    "from miniai.activations import *\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers.models.attention import Attention # AttentionBlock has been deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2de7765-8619-4951-8312-45692088c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "# Creating a tensor to represent a 16x16 image, with 32 channels and a batch size of 64 (NCHW)\n",
    "# NLP implementations call HxW (16x16) a sequence. Sequence mostly preceeds dimension / channel\n",
    "x = torch.randn(64, 32, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a60d375e-7bca-49cb-8d82-3856147caf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO replicate 1d-attention, we first need to flatten out the input tensor\n",
    "# in view(), -1 stands for 'everything else'. Transposing will give us NLP's equivalent of \n",
    "# NSD (BatchxChannelxDimension)\n",
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66656eb-8275-4c8b-9b0c-cae3ab1490e9",
   "metadata": {},
   "source": [
    ">Self-attention is a crucial mechanism in modern neural networks, especially within the context of natural language processing (NLP) and transformer architectures. It allows models to weigh the significance of different parts of an input sequence relative to each other, enabling the capture of complex dependencies and contextual relationships.\n",
    ">\n",
    "> ### Key Components of Self-Attention\n",
    ">\n",
    "> Self-attention operates using three main components derived from the input sequence:\n",
    ">\n",
    ">    **Query (Q)**: This vector represents the current focus or context for a specific word. It is generated through a linear transformation of the input embedding.\n",
    ">\n",
    ">    **Key (K)**: Each word in the input sequence has an associated key vector, which serves as a reference point. The model compares the query vector with all key vectors to determine relevance.\n",
    ">\n",
    ">    **Value (V)**: The value vectors hold the actual information content associated with each word. After calculating attention scores based on the similarity between queries and keys, these value vectors are weighted accordingly to produce the output.\n",
    ">\n",
    "> ### Process of Self-Attention\n",
    ">\n",
    ">    **Linear Transformations**: The input embeddings are transformed into three separate matrices—Q, K, and V—using learned weight matrices.\n",
    ">\n",
    ">    **Attention Scores Calculation**: The attention score for each pair of words is computed by taking the dot product of the query vector with all key vectors. This score indicates how much focus should be placed on each word when processing a particular word.\n",
    ">\n",
    ">    **Softmax Normalization**: The scores are normalized using a softmax function to create a probability distribution, ensuring that they sum to one.\n",
    ">\n",
    ">    **Weighted Sum**: The output for each word is obtained by calculating a weighted sum of the value vectors, where weights correspond to the normalized attention scores.\n",
    ">\n",
    ">    **Final Output**: The resulting context-aware representations are then passed through additional layers, typically including feed-forward neural networks, to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4e4b61-f600-4228-8ec9-e2ff1301a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input channels\n",
    "ni = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f2e566-4478-4f8d-83ae-74c7f8860c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need 3 projections for 32 in channels to 32 out channels\n",
    "# Creating simple linear layers (matmul plus a bias). Randomly initializing.\n",
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f86fd31-9994-412b-bdaf-88012707223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]),\n",
       " torch.Size([64, 256, 32]),\n",
       " torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For self attention, the technical parlance refers to these projections as keys, queries and values\n",
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)\n",
    "\n",
    "k.shape, q.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6257ed5-1648-45dc-beee-cbc41f2498f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matmul with the transpose. For every 64 items in the batch and for 256 pixels, we now have 256 weights\n",
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8efd48c-adf7-4283-9aac-5982166f4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a class for self attention. Note that this self-attention approach is more geared\n",
    "# towards resnets.\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm  = nn.GroupNorm(1, ni) # Basically, BatchNorm for sets of channels\n",
    "        self.q     = nn.Linear(ni, ni)\n",
    "        self.k     = nn.Linear(ni, ni)\n",
    "        self.v     = nn.Linear(ni, ni)\n",
    "        self.proj  = nn.Linear(ni, ni) # final projection to map items to different scales\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n, c, h, w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        # Matmul changes the scale of weights, normalizing to the original scale\n",
    "        s = (q@k.transpose(1, 2)) / self.scale\n",
    "        x = s.softmax(dim=1) @ v\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2).reshape(n,c,h,w) # reshaping back to the original\n",
    "        return x + inp # adding outputs to the original. Diffusers does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaca83fa-a690-413c-8926-0227e43bb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32) # self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95cb2d34-7a7f-4a2b-9817-f4324de5c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the self attention layer on the randomly generated numbers. Transpose ops above\n",
    "# ensure that the shape isn't changed\n",
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6a1c2a9-cc4e-43fb-bd06-cadd57db6713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9089,  1.4076,  0.8275, -2.1560,  0.6281, -1.2602, -0.0882, -1.6907,\n",
       "        -0.7971,  1.6068, -0.3910, -1.4385, -0.7531, -0.5945, -0.8377,  0.7390],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e84ad-1413-41bb-9278-872bf017a484",
   "metadata": {},
   "source": [
    "We need to be sure that our outputs align with Diffusers' `Attention` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31fc5808-64ea-4d9c-a2f4-a65864e04b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_params(a, b):\n",
    "    # Copy weights and biases from b to a\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf58ed9f-41dd-43c8-a731-4f2e8fbc7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffuser attention, updated\n",
    "at = Attention(32, dim_head=32, out_dim=32, norm_num_groups=1, residual_connection=1)\n",
    "# Comparing out q,k,v values to the ones from `at`\n",
    "src = sa.q, sa.k, sa.v, sa.proj, sa.norm\n",
    "dst = at.to_q, at.to_k, at.to_v, at.to_out[0], at.group_norm\n",
    "#  Zipping\n",
    "for s, d in zip(src, dst): cp_params(s, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0e5156e-f227-4274-9dfd-1db3d6b0e878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = at(x)\n",
    "rb[0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d93a5f-16d6-4c46-91fe-4e0bfaad81e3",
   "metadata": {},
   "source": [
    "`TODO - NOTE` **There appears to be a marginal difference between the two values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3a099a4-76d7-40ca-ba89-3ac30cdaf3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a984235-de66-44fc-afc4-620c86e13e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0074, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "328ea638-8e70-4c75-8a57-80e65c3395ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_to_batch(x, heads):\n",
    "    n, sl, d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    return x.transpose(2, 1).reshape(n*heads, sl, -1)\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n, sl, d = x.shape \n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1, sl, d*heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ae1a065-87a5-4ff6-ad41-7bc48164b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c71cf00b-b09a-46c6-a951-46245b55a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\n",
       "This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\n",
       "stack, concatenate and other operations.\n",
       "\n",
       "Examples for rearrange operation:\n",
       "\n",
       "```python\n",
       "# suppose we have a set of 32 images in \"h w c\" format (height-width-channel)\n",
       ">>> images = [np.random.randn(30, 40, 3) for _ in range(32)]\n",
       "\n",
       "# stack along first (batch) axis, output is a single array\n",
       ">>> rearrange(images, 'b h w c -> b h w c').shape\n",
       "(32, 30, 40, 3)\n",
       "\n",
       "# concatenate images along height (vertical axis), 960 = 32 * 30\n",
       ">>> rearrange(images, 'b h w c -> (b h) w c').shape\n",
       "(960, 40, 3)\n",
       "\n",
       "# concatenated images along horizontal axis, 1280 = 32 * 40\n",
       ">>> rearrange(images, 'b h w c -> h (b w) c').shape\n",
       "(30, 1280, 3)\n",
       "\n",
       "# reordered axes to \"b c h w\" format for deep learning\n",
       ">>> rearrange(images, 'b h w c -> b c h w').shape\n",
       "(32, 3, 30, 40)\n",
       "\n",
       "# flattened each image into a vector, 3600 = 30 * 40 * 3\n",
       ">>> rearrange(images, 'b h w c -> b (c h w)').shape\n",
       "(32, 3600)\n",
       "\n",
       "# split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n",
       ">>> rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape\n",
       "(128, 15, 20, 3)\n",
       "\n",
       "# space-to-depth operation\n",
       ">>> rearrange(images, 'b (h h1) (w w1) c -> b h w (c h1 w1)', h1=2, w1=2).shape\n",
       "(32, 15, 20, 12)\n",
       "\n",
       "```\n",
       "\n",
       "When composing axes, C-order enumeration used (consecutive elements have different last axis)\n",
       "Find more examples in einops tutorial.\n",
       "\n",
       "Parameters:\n",
       "    tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch).\n",
       "            list of tensors is also accepted, those should be of the same type and shape\n",
       "    pattern: string, rearrangement pattern\n",
       "    axes_lengths: any additional specifications for dimensions\n",
       "\n",
       "Returns:\n",
       "    tensor of the same type as input. If possible, a view to the original tensor is returned.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/miniai/lib/python3.11/site-packages/einops/einops.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rearrange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f06501c1-de02-4b4a-b9ce-362fe433330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = rearrange(t, 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42e15663-22ef-429a-855e-e3268777a612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)\n",
    "t2.shape, t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff5c5968-b225-4a52-94d3-b19664618719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34797c48-99d6-4929-a76e-7a41f3d1abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale  = math.sqrt(ni / nheads)\n",
    "        self.norm   = nn.BatchNorm2d(ni)\n",
    "        self.qkv    = nn.Linear(ni, ni*3)\n",
    "        self.proj   = nn.Linear(ni, ni)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        n, c, h, w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q, k, v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1, 2)) / self.scale\n",
    "        x = s.softmax(dim=-1) @ v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x).transpose(1, 2).reshape(n, c, h, w)\n",
    "        return x + inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fbb9b16-6124-4f3c-b62a-260b849d3d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4)\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffcd951e-cf7d-4ecc-95ac-7319a85e5ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0428, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0114, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.mean(), sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f3c9bb8-a3aa-41b2-a832-e7cd5e9272f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx, nmw = nm(t, t, t)\n",
    "nmx = nmx + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27d4cc37-fa5c-421c-92b9-6a4c35d1db7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0011, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0018, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmx.mean(), nmx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382012e-c76e-4b14-afa7-f8fd48db68ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
