{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75903bfa-1c5f-4e29-9b6b-40bc7af82e98",
   "metadata": {},
   "source": [
    "# **Moving to ResNets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f933f64-e933-49ed-af0f-55ffccd83911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, math, os, time, shutil\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fastcore.all as fc\n",
    "from pathlib import Path\n",
    "from operator import attrgetter, itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e70e976-6824-44f8-9090-4828c30976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bc57d-ea1f-4273-8cbf-fedc559b9347",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e2aee2-f14f-470f-a38c-3a431b1cde20",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl = 'image', 'label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 1024\n",
    "x_mean, x_std = 0.28, 0.35\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o) - x_mean) / x_std for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cade13d2-77e7-4867-8480-d4be44c3b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7dd7d6-4e20-45e0-a251-69eea32fef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations with our trusty General RelU\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53db48d-3b9d-4c7e-9345-6a30694ee724",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\n",
    "iw = partial(init_weights, leaky=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31473f4-3e79-4a5b-8c4d-2246d4f1bdec",
   "metadata": {},
   "source": [
    "## **Moving Toward Deeper Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc88b6-6ad3-4fc5-8849-8d9674368827",
   "metadata": {},
   "source": [
    "Prior to moving onto ResNets, we can experiment with increasing the depth of our ConvNet model. This should be possible since we were initializing our weights pretty effectively while using BatchNorm.\n",
    "\n",
    "Previously, we had a model which peaked at 64 output channels / filters. To increase output filters to 128, we can:\n",
    "1. Set the inputs and outputs to `conv(1, 8)` with a stride of 1.\n",
    "2. This would allow the network to be deeper and wider as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5df73-fc09-4e25-8785-324e0a8f57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison\n",
    "??get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbe3bf-a4f6-4079-92f8-de0acf3fc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the get_model() function for additional filters.\n",
    "def get_model(act=nn.ReLU, nfs=(1, 8, 16, 32, 64, 128), norm=nn.BatchNorm2d):\n",
    "    # initialize the first layer separately to have a wider and deeper network\n",
    "    #layers = [conv(1, 8, stride=1, act=act, norm=norm)]\n",
    "    layers = [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n",
    "    return nn.Sequential(*layers, conv(nfs[-1], 10, act=None, norm=norm, bias=True), nn.Flatten()).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47e08dd-d7e0-405e-a249-d729e5d1d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=(8,16,32,64,128), norm=nn.BatchNorm2d):\n",
    "    layers = [conv(1, 8, stride=1, act=act, norm=norm)]\n",
    "    layers += [conv(nfs[i], nfs[i+1], act=act, norm=norm) for i in range(len(nfs)-1)]\n",
    "    return nn.Sequential(*layers, conv(nfs[-1], 10, act=None, norm=norm, bias=True), nn.Flatten()).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e53688c-5142-4e32-a3be-c2a3dea95621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([8, 1, 3, 3]),\n",
       " torch.Size([8]),\n",
       " torch.Size([8]),\n",
       " torch.Size([8]),\n",
       " torch.Size([16, 8, 3, 3]),\n",
       " torch.Size([16]),\n",
       " torch.Size([16]),\n",
       " torch.Size([16]),\n",
       " torch.Size([32, 16, 3, 3]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]),\n",
       " torch.Size([64, 32, 3, 3]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64]),\n",
       " torch.Size([128, 64, 3, 3]),\n",
       " torch.Size([128]),\n",
       " torch.Size([128]),\n",
       " torch.Size([128]),\n",
       " torch.Size([10, 128, 3, 3]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_model()\n",
    "[o.shape for o in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b08ea-465b-4201-a0e6-70c809e935ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "lr, epochs = 6e-2, 5\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    " # Adding the scheduler\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0be7a-55a8-455c-80bd-ee7f16c0ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca792242-f752-41fd-a377-a737a6db36a2",
   "metadata": {},
   "source": [
    "We've hit **`91.9%`** with the simple addition of one line of code in `get_model()` which allows us to go from 8 to 128 filters. But we've also reached an inflection point i.e. adding additional width and depth will start becoming an exercise with diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d56d26-d704-4be3-b7b9-b93998976ae6",
   "metadata": {},
   "source": [
    "## **Skip Connections - The Core of ResNets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef5814-9859-464b-9a37-5e4e72714c58",
   "metadata": {},
   "source": [
    "ResNets (Residual Networks) were introduced by Kaiming He et al. in 2015, in the article [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385). \n",
    "\n",
    "![title](imgs/kaiming.png)\n",
    "\n",
    "The abstract states:\n",
    "\n",
    "> Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n",
    "\n",
    "Experiments showed that training deeper (56 layers vs. 20 layers) networks, began resulting in higher errors. The main insight was that _skip / shortcut / residual connections_ allowed deeper networks to train successfully - while retaining the training dynamics of much shallower networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bb204-1dac-4c3d-9124-19fc5974199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner convolutional block\n",
    "def _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n",
    "    # Change to stride 1\n",
    "    # NOTE that this block is different compared to the lecture.\n",
    "    # the init method, which initialized the batchnorm weights to 0. lead to worsening performance.\n",
    "    return nn.Sequential(conv(ni, nf, stride=1, act=act, norm=norm, ks=ks),\n",
    "                         conv(nf, nf, stride=stride, act=None, norm=norm, ks=ks))\n",
    "\n",
    "# NOTE that this block is different from the lecture\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "        super().__init__()\n",
    "        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm) # Convolutions\n",
    "        # nothing for condition, otherwise convolution with ni, nf and stride=1\n",
    "        # This changes the number of filters so that the shapes match.\n",
    "        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None) \n",
    "        # Condition in case there is a stride of 2, then pool the mean.\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "        self.act = act()\n",
    "    # The activation is applied to the result of the ResNet block.\n",
    "    # This is one approach to handling the block.\n",
    "    def forward(self, x): return self.act(self.convs(x) + self.idconv(self.pool(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230952df-730f-494f-aa46-edb79ad4bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=(8, 16, 32, 64, 128, 256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    # Previous convs in get_model() went from 128 channels to 10 channels, followed by flatten.\n",
    "    # However, the convs worked with a 1x1 input. Here, we flatten first and then use a linear layer.\n",
    "    # Conv on 1x1 input is equal to a linear layer.\n",
    "    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers).to(def_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447d483-0935-4131-8cfb-6e6e5b7ea889",
   "metadata": {},
   "source": [
    "Jeremy also mentioned that an important insight to convolutions is that a `conv()` on a 1x1 input is equal to a Linear layer. Based on my understanding, a convolution on a 1x1 input would reduce the kernel to the same size, effectively converting the matmul operation to much reduced form.\n",
    "\n",
    "The detailed explanation by ChatGPT is as follows:\n",
    "\n",
    "> A convolution on a 1×1 input tensor is equivalent to a linear (fully connected) layer because of the way the convolution operation works in this specific case:\n",
    "\n",
    "> Kernel Size Matches Input Size: When performing a convolution with a 1×1 input, the convolutional kernel also has dimensions 1×1. This means the kernel is applied to a single pixel at a time, and no spatial information is captured since there are no neighboring pixels.\n",
    "\n",
    "> Element-wise Multiplication and Sum: The convolution operation involves element-wise multiplication between the input and the kernel, followed by a summation of these products. Since both the input and kernel are 1×1, this operation reduces to simply multiplying the single input value by the kernel weight and potentially adding a bias term.\n",
    "\n",
    "> No Spatial Influence: In larger inputs, convolutional layers leverage spatial relationships by sliding the kernel over the input and combining information from neighboring pixels. However, with a 1×1 input, there are no neighboring pixels, so no spatial relationships are considered. This makes the operation functionally identical to a linear transformation, where the input is multiplied by a weight and added to a bias.\n",
    "\n",
    "> Equivalence to Linear Layer: The process described above is exactly what a linear (fully connected) layer does—each input is multiplied by a weight and summed with a bias. Therefore, a convolution on a 1×1 input is mathematically the same as applying a linear layer to that input.\n",
    "\n",
    ">In summary, a convolution on a 1×1 input is equivalent to a linear layer because it reduces to a simple weighted sum of the input, just like a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d52af-853e-4da0-b64d-3be13c87e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "[o.shape for o in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d992c98-09f4-4f13-865b-3717e53f9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to print out the shapes of our model architecture.\n",
    "def _print_shape(hook, mod, inp, outp): print(type(mod).__name__, inp[0].shape, outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8eba0-1987-4057-b911-d6bc4cb2348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, cbs=[DeviceCB(), SingleBatchCB()])\n",
    "# Using the Hooks context manager.\n",
    "with Hooks(model, _print_shape) as hooks: learn.fit(1, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1cf78-f682-4924-a893-f15cd026fcc2",
   "metadata": {},
   "source": [
    "`NOTE` There might be an issue with the Dataloader or the hooks class since the batch number is 2x the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33d07c-9156-400b-888b-126ca6619ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num Params|\\n|--|--|--|--|\\n'\n",
    "    tot = 0\n",
    "    def _f(hook, mod, inp, outp):\n",
    "        nonlocal res, tot\n",
    "        nparams = sum(o.numel() for o in mod.parameters())\n",
    "        tot += nparams\n",
    "        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparams}|\\n'\n",
    "    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, train=False, cbs=SingleBatchCB())\n",
    "    print(\"Total params: \", tot)\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else: print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e44c8f-8710-493e-82eb-59dbd9ba6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model(), dls, F.cross_entropy, cbs=DeviceCB()).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc89908-1cde-4677-a42f-9a4332281940",
   "metadata": {},
   "source": [
    "`NOTE` - the calculation is correct at the model level, the issue lies with the interaction between Hooks and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd2e7f-f047-4a02-9e0a-a43348dced0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e844c-f0ee-4f38-9722-e48a4da268ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde171b-9b85-46fe-8c10-a961df5480b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
