{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f800602-cc4e-4aad-94fd-0f2f5a3d057f",
   "metadata": {},
   "source": [
    "# **Denoising Diffusion Probabilistic Models with MiniAi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5ed1c-239e-4609-89d4-1722a1db82c0",
   "metadata": {},
   "source": [
    "Our custom training library should now allow us to begin building Stable Diffusion from scratch.\n",
    "\n",
    "We'll be working with the seminal 2020 paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM) which greatly simplified the training and generation process for these models which were [originally invented in 2015](https://arxiv.org/abs/1503.03585)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c378e95b-9c73-4080-9921-3615aa43f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,random,logging\n",
    "import fastcore.all as fc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c975238-ca42-4733-a5ca-81ab8f68a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863dc37-7181-4850-b662-93de9d3fa0b3",
   "metadata": {},
   "source": [
    "## **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfd79e4-c703-4bdc-98b2-8492bfb3e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 'image', 'label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b41bde-afba-4c04-90ae-d3a5672a2c4b",
   "metadata": {},
   "source": [
    "We will resize the `28x28` images to `32x32` to make life easier with regard to model architecures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9fc820c-d32f-47ca-ab4c-0c5d27d24ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [TF.resize(TF.to_tensor(o), (32, 32)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d159cdca-9afa-4f26-928c-6bb48248866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 32, 32]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "bs = 128\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=8)\n",
    "# Training data and batchify\n",
    "dt = dls.train\n",
    "xb, yb = next(iter(dt))\n",
    "xb.shape, yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f38d3-5b4c-4bfc-8a91-af462621824b",
   "metadata": {},
   "source": [
    "## **Create the DDP Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070bbe0-9db2-4f26-a60d-849f3d8865a8",
   "metadata": {},
   "source": [
    "We will be working with a [UNet](https://huggingface.co/docs/diffusers/en/api/models/unet2d), which in essence has an architecture which _\"consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.\"_\n",
    "\n",
    "It is important to note that we will be using the _unconditional model_ for this exercise. The model architecture is represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9831-3c6a-43eb-8bb6-6a74902472f5",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/blog/assets/78_annotated-diffusion/unet_architecture.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4057fd08-9748-4993-8075-4f01292db448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7007dc65-a640-4553-b594-624d55be41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4319c38-84d3-4a7f-80a6-019ab3436ef4",
   "metadata": {},
   "source": [
    "### **Training the Model Using a Callback**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b04d42-14ef-4146-8f78-cdcaa55348cb",
   "metadata": {},
   "source": [
    "The DDPM training process can be summarized as:\n",
    "\n",
    "1. Randomly select some timesteps in an iterative noising process.\n",
    "2. Add noise corresponding to the timestep to the original image.\n",
    "   - The variance of the noise increases with increasing timesteps.\n",
    "3. Pass the noisy image and the timestep to the model.\n",
    "4. Train the model, using an MSE loss between the model output and the amount of noise added to the image.\n",
    "5. Our callback executes steps 1 and 2 before setting up the input and ground truth tensors.\n",
    "6. The callback carries out steps 3 and 4, a sample is selected from this model which begins an iterative denoising process from pure noise.\n",
    "    - Predicted noise is removed by the model, using an expected noise schedule which is the reverse of what was used during the training process.\n",
    "  \n",
    "Against coding conventions, we will be using greek letters in the model callback to allow us to replicate the math as closely as possible. \n",
    "\n",
    "The final version's objects will have standard naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2317b68-e175-443f-a6f0-709781574ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMCB(TrainCB):\n",
    "    order = DeviceCB.order+1\n",
    "    def __init__(self, n_steps, beta_min, beta_max):\n",
    "        super().__init__()\n",
    "        self.n_steps,self.βmin,self.βmax = n_steps,beta_min,beta_max\n",
    "        # The variance schedule, linearly increased with timestep\n",
    "        self.β = torch.linspace(self.βmin, self.βmax, self.n_steps)\n",
    "        self.α = 1. - self.β \n",
    "        self.ᾱ = torch.cumprod(self.α, dim=0)\n",
    "        self.σ = self.β.sqrt()\n",
    "\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[0]).sample\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        device = learn.batch[0].device\n",
    "        ε = torch.randn(learn.batch[0].shape, device=device)  # noise, x_T\n",
    "        x0 = learn.batch[0] # original images, x_0\n",
    "        self.ᾱ = self.ᾱ.to(device)\n",
    "        n = x0.shape[0]\n",
    "        # select random timesteps\n",
    "        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)\n",
    "        ᾱ_t = self.ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "        xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε #noisify the image\n",
    "        # input to our model is noisy image and timestep, ground truth is the noise \n",
    "        learn.batch = ((xt, t), ε)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, sz):\n",
    "        device = next(model.parameters()).device\n",
    "        x_t = torch.randn(sz, device=device)\n",
    "        preds = []\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "            z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(device)\n",
    "            ᾱ_t1 = self.ᾱ[t-1]  if t > 0 else torch.tensor(1)\n",
    "            b̄_t = 1 - self.ᾱ[t]\n",
    "            b̄_t1 = 1 - ᾱ_t1\n",
    "            noise_pred = learn.model(x_t, t_batch).sample\n",
    "            x_0_hat = ((x_t - b̄_t.sqrt() * noise_pred)/self.ᾱ[t].sqrt()).clamp(-1,1)\n",
    "            x0_coeff = ᾱ_t1.sqrt()*(1-self.α[t])/b̄_t\n",
    "            xt_coeff = self.α[t].sqrt()*b̄_t1/b̄_t\n",
    "            x_t = x_0_hat*x0_coeff + x_t*xt_coeff + self.σ[t]*z\n",
    "            preds.append(x_t.cpu())\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d408d-903a-407f-ae99-ee1b7af76e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19808e-ad81-4efd-88d6-bf6c9cc07293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095d348-9d60-4f30-9228-a659e561e8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1e4d7-19a0-4da1-8d85-e5308d43c6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0d4a9-0dc3-4472-a402-cafb0bca6ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564765e-47fc-4d56-8cb8-36d6664ad6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209476c6-842e-4cd9-b15c-94b29e991d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ade577-c6ac-48fd-ae83-142aa4a11d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99470882-dd1a-4314-87a9-3c64b0f6ddaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
