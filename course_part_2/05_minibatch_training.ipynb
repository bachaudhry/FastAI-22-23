{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f676cc4-bbd5-4440-b310-fcb1e385a28e",
   "metadata": {},
   "source": [
    "# **Mini Batch Training for MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34c79cf2-95a9-442e-a250-b72d2f337628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, math, os, time, shutil\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch import tensor, nn\n",
    "import torch.nn.functional as F\n",
    "from fastcore.test import  test_close\n",
    "from pathlib import Path\n",
    "\n",
    "# Configs\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "# Path setup\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "# Loading MNIST data as tensors\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff73b2-644b-440e-b17d-e77da6044a76",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a1d4a-5b07-41e5-85e2-1b3942a0c4ca",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196455b3-48c1-468c-9169-8239bb421768",
   "metadata": {},
   "source": [
    "Copying over the starting cells from the previous NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a0c897c-1bd8-45fd-b7c3-bf49501109e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50\n",
    "\n",
    "n, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13cce843-3bae-41e6-95fe-ce2713be873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "349c4b70-ee05-4966-b992-ee7374689db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh,  10)\n",
    "pred = model(x_train)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89a952a6-ce01-4fce-9f43-1a48125b127e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.09, -0.21, -0.08,  ..., -0.03,  0.01,  0.06],\n",
       "        [-0.07, -0.14, -0.14,  ...,  0.03,  0.04,  0.14],\n",
       "        [-0.19, -0.04,  0.02,  ..., -0.01, -0.00,  0.02],\n",
       "        ...,\n",
       "        [-0.03, -0.22, -0.04,  ..., -0.01,  0.09,  0.14],\n",
       "        [-0.10, -0.09, -0.05,  ..., -0.01,  0.02,  0.11],\n",
       "        [-0.03, -0.25, -0.06,  ...,  0.00,  0.03,  0.14]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114894b6-6128-4c2e-927f-a45d92fa3945",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e3b87-8ce6-4a6e-a393-b90c42ce010b",
   "metadata": {},
   "source": [
    "We need to improve our loss function from before. Instead of outputting 1 number per image, we will now have 10 one-hot-encoded numbers per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bb69627-8cd9-410f-b834-9db67fa003a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a793025d-65b5-487c-8b91-88b9ae19661b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad7d82-9266-426d-8371-8a4f2df983f7",
   "metadata": {},
   "source": [
    "Using the formula: $$ \\log\\left(\\frac{a}{b}\\right) = \\log(a) - \\log(b)$$ allows us to simplify the `log_softmax()` function further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30aea25b-d0d4-4b32-993c-5caecadb47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c5f0-021d-4639-b7b0-bfa6f3a56fa1",
   "metadata": {},
   "source": [
    "Also, we can simplify things even further by using the [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp) trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57afe8a4-b33b-468a-96d3-37b03378ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    # Taking the max on the last dim\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x - m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b09f0b3-6e24-41cb-aea3-568463afe12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting log_softmax() using logsumexp() from PyTorch\n",
    "def log_softmax(x): return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d532b6fc-777a-4660-9587-e377626e4bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_close(logsumexp(pred), pred.logsumexp(-1))\n",
    "\n",
    "sm_pred = log_softmax(pred)\n",
    "sm_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1a09c-1ba3-4f6d-9e96-86af54fde523",
   "metadata": {},
   "source": [
    "We can index into our 1-hot encoded x's using PyTorch's (and, NumPy's) advanced indexing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b182ed25-ae42-4bef-9e57-4f52e46503cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pick a sample\n",
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04e70482-d00e-475d-8ddd-fe856dab33b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.20, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.37, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.36, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the positioning of the indices\n",
    "sm_pred[0, 5], sm_pred[1, 0], sm_pred[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17d2bf08-1595-4978-a62a-8ca0bcb93370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexing method allows us to get these values as follows\n",
    "sm_pred[[0, 1, 2], y_train[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbf8c44b-aca5-4b35-bc87-53d9aabe7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating negative log likelihood loss\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9c3092e-6af7-4860-b19a-f2e311c3b38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f06609e4-e736-4e04-839f-e4b8513d260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's version\n",
    "test_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63a61b-193b-4e60-a394-debc19db0a36",
   "metadata": {},
   "source": [
    "`F.log_softmax` and `F.nll_loss` are combined in one function called `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99465b6e-191d-40d8-839b-0aa2c0dfa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474341a-57d9-40f4-bf4d-2a758df71470",
   "metadata": {},
   "source": [
    "## Basic Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e92fd557-abd0-43f1-9477-9c8c01fddca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our loss function\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8224c0aa-f856-4bca-9f8f-86f9f42e91e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 50               # batch size\n",
    "\n",
    "xb = x_train[0 : bs]  # Mini batch from training data\n",
    "preds = model(xb)     \n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "402e6fbc-73d2-4013-a23c-47b3bb782b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7,\n",
       "        6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking our target, matching the mini-batch size\n",
    "yb = y_train[0 : bs]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6af71020-c66a-4daa-b4c4-280f2522bfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the loss function\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9898f02a-d345-42dd-aa4e-164aa4484f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3,\n",
       "        9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee7f3497-9fdc-4e2c-a6db-abcf92d13e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.08)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the accuracy of our predictions\n",
    "def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\n",
    "\n",
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ba660fd-d1e2-4f99-b376-11bde0300159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a learning rate and number of epochs\n",
    "lr = 0.5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4176e717-4661-4567-afbe-6dbffe73a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b460bd3-1b88-422b-b512-b0f8c6b18211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30, 0.08\n"
     ]
    }
   ],
   "source": [
    "xb, yb = x_train[:bs], y_train[:bs]\n",
    "preds = model(xb)\n",
    "report(loss_func(preds, yb), preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6272f4b8-986c-48a7-ab11-53a2f632a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12, 0.98\n",
      "0.12, 0.94\n",
      "0.08, 0.96\n"
     ]
    }
   ],
   "source": [
    "# Our training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162b311-a45c-48e5-b178-332f54697e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
