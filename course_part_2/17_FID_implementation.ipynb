{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbc01de-4686-4465-8d53-6c425a64ba0b",
   "metadata": {},
   "source": [
    "# **Implementation of Frechet Inception Distance (FID) and Kernel Inception Distance (KID)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef670b7-2dfc-491a-8d8f-aa3e5f34c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,random\n",
    "import fastcore.all as fc\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from scipy import linalg\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torchvision.transforms.functional as TF, torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *\n",
    "from miniai.accel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a2f48d-21f3-4ade-8e79-aa601c5912a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "from torch import distributions\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a4217-ea65-4ac2-b28a-3509d0d37fb7",
   "metadata": {},
   "source": [
    "## **Evaluating Generative Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095bd3c-818d-403a-bad1-b0cb0b72d924",
   "metadata": {},
   "source": [
    "We're reaching a point where our generated images are getting good enough to actually bias our own perception of what a _good image_ actually looks like. We need to be able to measure the quality of these generated images against a real life benchmark. The benchmark is an actual human being's comparison of generated images and a set of real images using the good old \"mark-one eyeball\" approach.\n",
    "\n",
    "While the research community is constantly finding new ways to mathematically measure the difference between the two, the most commonly used metric is called the [Frechet Inception Distance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance).\n",
    "\n",
    "> The Frechet Inception Distance score, or FID for short, is a metric that calculates the distance between feature vectors calculated for real and generated images.\n",
    ">\n",
    ">The score summarizes how similar the two groups are in terms of statistics on computer vision features of the raw images calculated using the inception v3 model used for image classification. Lower scores indicate the two groups of images are more similar, or have more similar statistics, with a perfect score being 0.0 indicating that the two groups of images are identical.\n",
    ">\n",
    ">The FID score is used to evaluate the quality of images generated by generative adversarial networks, and lower scores have been shown to correlate well with higher quality images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cd611-a628-4552-9aa7-89cc387ca4eb",
   "metadata": {},
   "source": [
    "## **Setup CLassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa54095-fcdf-42aa-bd7f-2a08c4224540",
   "metadata": {},
   "source": [
    "To implement FID, we will use our existing model `fashion_ddpm_mp` which was trained using mixed precision. Here we will refer to this model as the `sample model` or `smodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a7fd53-6f05-4bb9-808c-86614e7e2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl = 'image', 'label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 512\n",
    "\n",
    "# `*2-1` ensures the image range is normalized to (-1, 1) from NB 14 - Section AUGMENT 2\n",
    "# This was to move away from subtracting the mean and dividing the standard deviation\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a3e4e6-dcbf-4747-8007-c259e9e7129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = xb, yb = next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924752a5-b4af-469a-ae78-41195ff6bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [DeviceCB(), MixedPrecision()]\n",
    "model = torch.load('models/data_aug2.pkl')\n",
    "learn = Learner(model, dls, F.cross_entropy, cbs=cbs, opt_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1a517-68ed-4f87-b12a-463ccfd06e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_outp(hook, mode, inp, outp):\n",
    "    if not hasattr(hook, 'outp'): hook.outp = []\n",
    "    hook.outp.append(to_cpu(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51d994-e90f-481a-ab2c-7f9530839ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hcb = HooksCallback(append_outp, mods=[learn.model[6]], on_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b26f34-1901-4d69-a8f4-2180cef92d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, train=False, cbs=[hcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4a189-4113-4b88-a56e-16f266543472",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = hcb.hooks[0].outp[0].float()[:64]\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19438dd-6f3a-4304-9dc1-2f95f02663d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(learn.model[8])\n",
    "del(learn.model[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bee68a-5ebf-4bb5-ad22-211b2ec79fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats, y = learn.capture_preds()\n",
    "feats = feats.float()\n",
    "feats.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba97ed1-c2bf-467a-b820-f59208210da9",
   "metadata": {},
   "source": [
    "## **Calculate FID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fe2db-d60b-44ab-b0a3-b34ddfedfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_min, beta_max, n_steps = 0.0001, 0.02, 1000\n",
    "beta = torch.linspace(beta_min, beta_max, n_steps)\n",
    "alpha = 1.-beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922ad7c-e8b1-4629-b41f-8cffd97d62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrying over the noisify function from before\n",
    "def noisify(x0, ᾱ):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\n",
    "\n",
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59bb1c-87d7-4321-9742-ff5bdafa16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2 = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba85e5a-ade3-4036-92ca-853338fe6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x): return super().forward(*x).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d46b32-6f1f-482d-85c4-014d21f2b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample model\n",
    "smodel = torch.load('models/fashion_ddpm_mp.pkl').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410b88e-7aa7-431c-a1d3-e532849caa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, sz, alpha, alphabar, sigma, n_steps):\n",
    "    device = next(model.parameters()).device\n",
    "    x_t = torch.randn(sz, device=device)\n",
    "    preds = []\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "        z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(device)\n",
    "        ᾱ_t1 = alphabar[t-1]  if t > 0 else torch.tensor(1)\n",
    "        b̄_t = 1 - alphabar[t]\n",
    "        b̄_t1 = 1 - ᾱ_t1\n",
    "        x_0_hat = ((x_t - b̄_t.sqrt() * model((x_t, t_batch)))/alphabar[t].sqrt())\n",
    "        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n",
    "        preds.append(x_0_hat.cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3de63-d8c3-42a7-9582-93a20b55a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = sample(smodel, (256, 1, 32, 32), alpha, alphabar, sigma, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f67a4-0a29-41c1-80e7-2cc733fa6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = samples[-1]*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d9c75-9d3c-4a3f-b7bb-b1b18be07276",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(s[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb0cf6-a2fa-4d3a-bcb6-4c8a6efd04ad",
   "metadata": {},
   "source": [
    "Taking 4 of the previously trained model's generated samples, we look at some statistics of select activations. Recall that we created `summary()` for the `TrainLearner`, which returns various output shapes of each layer of our model.\n",
    "\n",
    "![title](imgs/table_summary.png)\n",
    "\n",
    "We will take our samples and run them through the model which is pretrained to predict fashion classes. Afterwards, the layer titled `GlobalAvgPool`, will be extracted and the mean of each of the channel across the batch size will calculated.\n",
    "\n",
    "Channels would contain different feature characteristics from the fashion dataset, so the mean would be representative of the distributions of these characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ddeec-4602-4004-8a48-56ac64e7b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataLoader which contains no training batches. It does contain one\n",
    "# validation batch with the samples from above.\n",
    "clearn = TrainLearner(model, DataLoaders([], [(s, yb)]), loss_func=fc.noop, cbs=[DeviceCB()], opt_func=None)\n",
    "feats2, y2 = clearn.capture_preds()\n",
    "feats2 = feats2.float().squeeze()\n",
    "feats2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448cb77-1baf-40e5-8ce0-329c0d6b72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "means = feats.mean(0)\n",
    "means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3baee0-fe93-4d96-a7ea-b3b663ebd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = feats.T.cov()\n",
    "covs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6976c-cb28-4d90-bbe1-d244fd4bdbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67048bde-53a0-4091-916a-36617b059cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a98416-e9c3-4480-8bc2-59a281c17603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58908652-ae5f-4a1b-851d-d3cd90dd5b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ef801-6ba2-4ff6-aad6-02ae8620f6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf26cc6-b6e2-4e26-9e37-462ac6b29bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2669484-22e6-483e-a370-412f7761cd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0cb81-112f-4f2a-8903-e74736ecb84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b13bde-e556-4727-bd03-8fef7fc998c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55eb8a-e29f-4f9c-969a-9bd9feab574d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a962bb-dd29-43c6-889e-b6aadf2317e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd9ffb-6f93-4605-b025-24534cf896b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
